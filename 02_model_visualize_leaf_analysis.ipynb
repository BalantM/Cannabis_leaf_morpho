{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#################### IMPORT MODULES ####################\n",
    "########################################################\n",
    "\n",
    "import numpy as np ## for working with arrays\n",
    "import matplotlib.pyplot as plt ## for plotting\n",
    "%matplotlib inline\n",
    "import math ## for math\n",
    "from scipy.interpolate import interp1d ## for interpolating points\n",
    "import pandas as pd # for dataframe capabilities\n",
    "import seaborn as sns # for hue and other plotting capabilities\n",
    "\n",
    "# for reading in lists of files in folders \n",
    "from os import listdir \n",
    "from os.path import isfile, join\n",
    "\n",
    "# for principal component analysis\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "# for Kruskal-Wallis test\n",
    "from scipy import stats\n",
    "\n",
    "## for linear discriminant analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# for Spearman Rank correlation\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "## suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#################### DEFINE FUNCTIONS ####################\n",
    "##########################################################\n",
    "\n",
    "## function to find indices of a character in a string\n",
    "\n",
    "def find(s, ch):\n",
    "    return [i for i, ltr in enumerate(s) if ltr == ch]\n",
    "\n",
    "# define a function to return equally spaced, interpolated points for a given polyline\n",
    "# inputs: arrays of x and y values for a polyline, number of points to interpolate\n",
    "# ouputs: interpolated points along the polyline, inclusive of start and end points, as arrays\n",
    "\n",
    "def interpolation(x, y, number): \n",
    "\n",
    "    distance = np.cumsum(np.sqrt( np.ediff1d(x, to_begin=0)**2 + np.ediff1d(y, to_begin=0)**2 ))\n",
    "    distance = distance/distance[-1]\n",
    "\n",
    "    fx, fy = interp1d( distance, x ), interp1d( distance, y )\n",
    "\n",
    "    alpha = np.linspace(0, 1, number)\n",
    "    x_regular, y_regular = fx(alpha), fy(alpha)\n",
    "    \n",
    "    return x_regular, y_regular\n",
    "\n",
    "# define a function to find corresponding index positions of landmarks in the trace file\n",
    "# and return a trace file at higher resolution with more interpolated points\n",
    "# inputs: arrays with x and y coordinates for the landmarks and trace, total resolution to interpolate over\n",
    "# outputs: the indices of the landmarks in the returned modified trace files, now at specified resolution,\n",
    "# and x and y coordinates at higher resolution as arrays\n",
    "\n",
    "def highres_landmarking(landmarks_file, trace_file, resolution):\n",
    "    \n",
    "    trace_xvals = trace_file[:,0] # specify trace xvals\n",
    "    trace_yvals = trace_file[:,1] # specify trace yvals\n",
    "\n",
    "    trace_xvals, trace_yvals = interpolation(trace_xvals,trace_yvals,resolution) # interpolate points to achieve high resolution\n",
    "\n",
    "    highres_xvals = trace_xvals # copy trace xvals to replace its trace vals with landmark vals\n",
    "    highres_yvals = trace_yvals # copy trace yvals to replace its trace vals with landmark vals\n",
    "\n",
    "    land_indices = [] # list to store index values of landmarks in trace\n",
    "\n",
    "    for i in range(landmarks_file.shape[0]): # for each landmark\n",
    "\n",
    "        landx = landmarks_file[i,0] # select current landmark x val\n",
    "        landy = landmarks_file[i,1] # select current landmark y val\n",
    "\n",
    "        distances = [] # list to store distances of current landmark with each trace coord\n",
    "\n",
    "        for j in range(len(trace_xvals)): # for each trace coord\n",
    "\n",
    "            tracex = trace_xvals[j] # current trace x val\n",
    "            tracey = trace_yvals[j] # current trace y val\n",
    "\n",
    "            d = np.sqrt( (landx-tracex)**2 + (landy-tracey)**2 ) # find distance to each landmark for each trace\n",
    "\n",
    "            distances.append(d) # append distance of landmark to each trace val\n",
    "\n",
    "        min_val = np.min(distances) # find min distance value of a trace for the landmark\n",
    "        min_ind = distances.index(min_val) # find min index value among trace coords to the landmark\n",
    "\n",
    "        land_indices.append(min_ind) # append trace index that corresponds to landmark\n",
    "        \n",
    "    return land_indices, highres_xvals, highres_yvals\n",
    "\n",
    "# define a function to find the angle between 3 points anti-clockwise in degrees, p2 being the vertex\n",
    "# and p1 being the left hand of the angle\n",
    "# inputs: three angle points, as tuples\n",
    "# output: angle in degrees\n",
    "\n",
    "def angle_between(p1, p2, p3):\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    x3, y3 = p3\n",
    "    deg1 = (360 + math.degrees(math.atan2(x1 - x2, y1 - y2))) % 360\n",
    "    deg2 = (360 + math.degrees(math.atan2(x3 - x2, y3 - y2))) % 360\n",
    "    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)\n",
    "\n",
    "# define a function to rotate 2D x and y coordinate points around the origin\n",
    "# inputs: x and y vals (can take pandas dataframe columns) and the degrees (positive, anticlockwise) to rotate\n",
    "# outputs: rotated and y vals\n",
    "\n",
    "def rotate_points(xvals, yvals, degrees):\n",
    "    \n",
    "    angle_to_move = 270 - degrees # USE 270 DEGREES TO ORIENT TIP DOWNWARD\n",
    "    rads = np.deg2rad(angle_to_move)\n",
    "    \n",
    "    new_xvals = xvals*np.cos(rads)-yvals*np.sin(rads)\n",
    "    new_yvals = xvals*np.sin(rads)+yvals*np.cos(rads)\n",
    "    \n",
    "    return new_xvals, new_yvals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#################### DEFINE PARAMETERS ####################\n",
    "###########################################################\n",
    "\n",
    "## specify folder to read in outlines\n",
    "out_path = \"./03_out/\"\n",
    "\n",
    "## specify folder to read in landmarks\n",
    "land_path = \"./04_land/\"\n",
    "\n",
    "## specify folder to read out plots\n",
    "plot_path = \"./05_plot/\"\n",
    "\n",
    "## specify number of interpolated points to use initially\n",
    "resolution = 10000 \n",
    "\n",
    "## number of landmarks to interpolate over on each side of the lobe\n",
    "num_land = 200\n",
    "\n",
    "## set the degrees of the polynomial\n",
    "degrees = 2\n",
    "\n",
    "## select the number of points to interpolate between angles of theoretical leaves\n",
    "num_inter_points = 100 \n",
    "\n",
    "## select the new number of theoretical lobes to use\n",
    "new_lobe_num = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File names\n",
    "\n",
    "The names of the files for this next step need to have the format: \n",
    "\n",
    "land/out_population_individual number_total node number_node number and letter_lobe number (example: out_AM15_3_8_4b_5.txt)\n",
    "\n",
    "If your files have different name structure you will need to change the GET LEAF INFO part of the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#################### GET FILE NAMES OF LEAVES ####################\n",
    "##################################################################\n",
    "\n",
    "## retrieve a list of leaf file names\n",
    "leaf_files = [f for f in listdir(out_path) if isfile(join(out_path, f))] \n",
    "\n",
    "## sort the file names alphanumerically \n",
    "leaf_files.sort()\n",
    "\n",
    "## retrieve leaf names\n",
    "cannabis_leaves = []\n",
    "\n",
    "for i in range(len(leaf_files)):\n",
    "    \n",
    "    cannabis_leaves.append(leaf_files[i][4:])\n",
    "    \n",
    "## remove \"Store\" file\n",
    "if \"Store\" in cannabis_leaves:\n",
    "    cannabis_leaves.remove('Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of x vals of theoretical leaf with new lobe number\n",
    "theor_xvals = []\n",
    "\n",
    "# a list of y vals of theoretical leaf with new lobe number\n",
    "theor_yvals = []\n",
    "\n",
    "# create lists to store information about leaves\n",
    "pop_list = []\n",
    "ind_list = []\n",
    "tot_list = []\n",
    "nod_list = []\n",
    "nodab_list = []\n",
    "rel_list = []\n",
    "lob_list = []\n",
    "\n",
    "for c in cannabis_leaves:\n",
    "    \n",
    "    # get current leaf file name\n",
    "    file = c\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    #######################################################\n",
    "    #################### GET LEAF INFO ####################\n",
    "    #######################################################\n",
    "\n",
    "    ## get underscore indices\n",
    "    underscores = find(file, \"_\")\n",
    "\n",
    "    ## get population (str)\n",
    "    population = file[0:underscores[0]]\n",
    "    pop_list.append(population)\n",
    "\n",
    "    ## get individual (str) *corrected\n",
    "    individual = file[0:underscores[1]]\n",
    "    ind_list.append(individual)\n",
    "\n",
    "    ## get total node number (int)\n",
    "    total_num = int(file[(underscores[1]+1):underscores[2]])\n",
    "    tot_list.append(total_num)\n",
    "\n",
    "    ## get node ID (str)\n",
    "    node_id = file[(underscores[2]+1):underscores[3]]\n",
    "\n",
    "    ## get node number (int)\n",
    "    node_num = int(node_id[0:-1])\n",
    "    nod_list.append(node_num)\n",
    "\n",
    "    ## get relative node number (float)\n",
    "    rel_node = node_num/total_num\n",
    "    rel_list.append(rel_node)\n",
    "\n",
    "    ## get node rep (str)\n",
    "    node_rep = node_id[-1]\n",
    "    nodab_list.append(node_rep)\n",
    "\n",
    "    ## get lobe number (int)\n",
    "    lobe_num = int(file[(underscores[3]+1):-4])\n",
    "    lob_list.append(lobe_num)\n",
    "\n",
    "    ######################################################\n",
    "    #################### READ IN DATA ####################\n",
    "    ######################################################\n",
    "\n",
    "    ## read in data\n",
    "    blade_trace = np.loadtxt(out_path+\"out_\"+file)\n",
    "    landmarks_readin = np.loadtxt(land_path+\"land_\"+file)\n",
    "\n",
    "    ###########################################################\n",
    "    #################### REORDER LANDMARKS ####################\n",
    "    ###########################################################\n",
    "\n",
    "    ## reorder the landmarks file\n",
    "    # read in as lobe starts and ends\n",
    "    # followed by tips\n",
    "    # and ending with petiolar junction\n",
    "    # to reorder we want: for each lobe start, tip, and end and petiolar junction separate at the end\n",
    "\n",
    "    ## get petiolar junciton coordinates\n",
    "    pet_junc = landmarks_readin[-1]\n",
    "\n",
    "    ## get lobe base coordinates\n",
    "    lobe_bases = landmarks_readin[0:lobe_num*2]\n",
    "\n",
    "    ## get lobe start coordinates\n",
    "    lobe_starts = lobe_bases[0::2]\n",
    "\n",
    "    ## get lobe end coordinates\n",
    "    lobe_ends = lobe_bases[1::2]\n",
    "\n",
    "    ## get lobe tip coordinates\n",
    "    lobe_tips = landmarks_readin[lobe_num*2:-1]\n",
    "\n",
    "    ## reorder landmarks file\n",
    "    landmarks = np.zeros((lobe_num*3, 2))\n",
    "\n",
    "    for i in range(lobe_num):\n",
    "\n",
    "        landmarks[i*3] = lobe_starts[i]\n",
    "        landmarks[i*3+1] = lobe_tips[i]\n",
    "        landmarks[i*3+2] = lobe_ends[i]\n",
    "\n",
    "    ############################################################################################\n",
    "    #################### FIND LANDMARK INDICES AND CALCULATE HIGH RES TRACE ####################\n",
    "    ############################################################################################\n",
    "\n",
    "    # find landmark indices in the trace and return at higher resolution\n",
    "\n",
    "    land_indices, highres_xvals, highres_yvals = highres_landmarking(landmarks, blade_trace, resolution)\n",
    "\n",
    "    ##################################################################\n",
    "    #################### ROTATE, TRANSLATE, SCALE ####################\n",
    "    ##################################################################\n",
    "\n",
    "    # rotate so that tip faces downward\n",
    "    # translate so that the petiolar junction is at the origin\n",
    "    # scale so that the central lobe is length of 1\n",
    "\n",
    "    highres_coords = np.column_stack((highres_xvals, highres_yvals)) # create a 2D array of x and y coords\n",
    "\n",
    "    tip_ind = land_indices[int( ((len(land_indices)/2) + 0.5) - 1 )] # find the tip index value\n",
    "    start_inds = land_indices[0::3] # find the start index value of each lobe\n",
    "    end_inds = land_indices[2::3] # find the end index value of each lobe\n",
    "    tip = highres_coords[int(tip_ind)] # get tip coordinates\n",
    "\n",
    "    # calculate angle between tip, petiolar junction, and a line extending right of petiolar junciton\n",
    "    angle = angle_between(tip, pet_junc, (pet_junc[0]+1,pet_junc[1]) ) \n",
    "\n",
    "    # rotate leaf so that tip points downwards\n",
    "    rotated_xvals, rotated_yvals = rotate_points(highres_coords[:,0],highres_coords[:,1], angle)\n",
    "\n",
    "    # calculate coordinates of new rotated petiolar junction to translate leaf to the origin\n",
    "    origin_x, origin_y = rotate_points(pet_junc[0],pet_junc[1], angle)\n",
    "\n",
    "    # translate leaf to the origin\n",
    "    trans_xvals, trans_yvals = rotated_xvals - origin_x, rotated_yvals - origin_y\n",
    "\n",
    "    # calculate new tip points to scale central lobe legnth to 1\n",
    "    tip_x, tip_y = trans_xvals[int(tip_ind)], trans_yvals[int(tip_ind)]\n",
    "\n",
    "    # calculate central lobe length, from tip to origin\n",
    "    central_lobe_len = np.sqrt( (tip_x-0)**2 + (tip_y-0)**2 )\n",
    "\n",
    "    # calculate scaled coordinate values\n",
    "    scaled_xvals, scaled_yvals = trans_xvals/central_lobe_len, trans_yvals/central_lobe_len\n",
    "\n",
    "    # stack scaled coordinates back into a 2D array\n",
    "    leaf_coords = np.column_stack((scaled_xvals, scaled_yvals)) # create a 2D array of x and y coords\n",
    "\n",
    "    #############################################################\n",
    "    #################### REINDEX COORDINATES ####################\n",
    "    #############################################################\n",
    "\n",
    "    # reindex the coordinate values so that they begin with the first landmark\n",
    "\n",
    "    start_coordinate = leaf_coords[land_indices[0]] # save beginning coordinate to double check results\n",
    "\n",
    "    num_coords = np.shape(leaf_coords)[0] # get number of coordinate values\n",
    "    zero_index = land_indices[0] # get the zeroth index to start at\n",
    "\n",
    "    reindexed_arr = np.zeros((num_coords,2)) # array to store new vals\n",
    "\n",
    "    for i in range(num_coords):\n",
    "\n",
    "        curr_ind = i # get current index\n",
    "        new_ind = (i - zero_index)%num_coords # get the new index to use\n",
    "        reindexed_arr[new_ind,:] = leaf_coords[curr_ind,:] # store the value at the new index position\n",
    "\n",
    "    # print(start_coordinate, \"\\n\", reindexed_arr[0]) # double check that the start value is now at position 0\n",
    "\n",
    "    ###########################################################\n",
    "    #################### REINDEX LANDMARKS ####################\n",
    "    ###########################################################\n",
    "\n",
    "    # similar to above, reindex landmarks values the same way\n",
    "\n",
    "    num_coords = np.shape(leaf_coords)[0] # get number of coordinate values\n",
    "    zero_index = land_indices[0] # get the zeroth index to start at\n",
    "\n",
    "    reindexed_landmarks = [] # a list to store new landmark indices\n",
    "\n",
    "    for i in range(len(land_indices)):\n",
    "\n",
    "        curr_ind_val = land_indices[i] # get current index\n",
    "        new_ind_val = (curr_ind_val - zero_index)%num_coords # get the new index to use\n",
    "        reindexed_landmarks.append(new_ind_val)\n",
    "\n",
    "    # print(reindexed_landmarks) # double check that results worked\n",
    "\n",
    "    #####################################################################################\n",
    "    #################### INTERPOLATE AND CONVERT TO POLAR COORDINATES ####################\n",
    "    #####################################################################################\n",
    "\n",
    "    tip_index = num_land - 1 # this will be the index of the tip\n",
    "\n",
    "    polar_lobe_angles = [] # a list of the angle of each lobe, from its tip to origin\n",
    "    polar_angles = [] # a list of lists of the angle values for each lobe point\n",
    "    polar_radii = [] # a list of lists of the radii values for each lobe point\n",
    "\n",
    "    for j in range(lobe_num): # for the number of lobes\n",
    "\n",
    "        lobe_index = j # the index of the lobe to retrieve, starting at 0\n",
    "\n",
    "        start_indices = reindexed_landmarks[0::3] # get the lobe start indices\n",
    "        tip_indices = reindexed_landmarks[1::3] # get the lobe tip indices\n",
    "        end_indices = reindexed_landmarks[2::3] # get the lobe end indices\n",
    "\n",
    "        curr_start = start_indices[lobe_index] # for the current lobe, get the start index\n",
    "        curr_tip = tip_indices[lobe_index] # for the current lobe, get the tip index\n",
    "        curr_end = end_indices[lobe_index] # for the current lobe, get the end index\n",
    "\n",
    "        right_side = reindexed_arr[curr_start:curr_tip+1] # get the right side of the lobe \n",
    "        left_side = reindexed_arr[curr_tip:curr_end+1] # get the left side of the lobe\n",
    "\n",
    "        # interpolate pseudolandmarks on right side\n",
    "        right_inter_x, right_inter_y = interpolation(right_side[:,0], right_side[:,1], num_land)\n",
    "        # interpolate pseudolandmarks on left side\n",
    "        left_inter_x, left_inter_y  = interpolation(left_side[:,0], left_side[:,1], num_land) \n",
    "        \n",
    "        right_inter = np.column_stack((right_inter_x, right_inter_y)) # recombine right coords\n",
    "        right_inter = right_inter[0:-1,:] # remove the last value from the last side to avoid duplicate with left\n",
    "        left_inter = np.column_stack((left_inter_x, left_inter_y)) # recombine left coords\n",
    "\n",
    "        lobe_pseudos = np.row_stack((right_inter, left_inter)) # combine all lobe coords together\n",
    "\n",
    "        lobe_tip = lobe_pseudos[tip_index,:] # set the point of the lobe tip\n",
    "        lobe_origin = (0,0) # set the point of the petiolar junction, which is the origin\n",
    "        ref_point = (0,1) # set the reference point\n",
    "\n",
    "        polar_lobe_angles.append(angle_between(lobe_tip, lobe_origin, ref_point)) # get overall lobe angle and append to list\n",
    "\n",
    "        lobe_angles = [] # list to store angles of each lobe point\n",
    "        lobe_radii = [] # list to store radii of each lobe point\n",
    "\n",
    "        for i in range(len(lobe_pseudos)): # for the number of points in the lobe\n",
    "\n",
    "            curr_point = lobe_pseudos[i,:] # get the current point\n",
    "\n",
    "            lobe_angles.append(angle_between(curr_point, lobe_origin, ref_point)) # get point angle and append to list\n",
    "\n",
    "            lobe_radii.append(np.sqrt((curr_point[0]-0)**2 + (curr_point[1]-0)**2)) # get point radius and append to list\n",
    "\n",
    "        polar_angles.append(lobe_angles) # append current lobe angles to polar angles list\n",
    "        polar_radii.append(lobe_radii) # append current lobe radii to polar radii list\n",
    "\n",
    "    polar_lobe_angles.insert(0,0) # add 0 at beginning for intervals \n",
    "    polar_lobe_angles.append(360) # add 360 at end for intervals \n",
    "\n",
    "    ##########################################################\n",
    "    #################### MODEL POLYNOMIAL ####################\n",
    "    ##########################################################\n",
    "\n",
    "    coefficients = [] # a list to store the coefficients for each polynomial for each, coeff stored as 1D arrays \n",
    "\n",
    "    for i in range(len(polar_angles[0])): # for the number of points in each lobe\n",
    "\n",
    "        curr_point_ind = int(i) # get current landmark index\n",
    "\n",
    "        curr_angles = [] # list to store current angles of current landmark\n",
    "        curr_radii = [] # list to store current radii of current landmark\n",
    "\n",
    "        for j in range(len(polar_lobe_angles)-2): # for the number of lobes, -2 because we added 0 and 360\n",
    "\n",
    "            curr_angles.append(polar_angles[j][i]) # for lobe j, get angle i\n",
    "            curr_radii.append(polar_radii[j][i]) # for lobe j, get radius i\n",
    "\n",
    "        # calculate polynomial function for the current landmark and get coefficients\n",
    "        coef = np.polyfit(curr_angles, curr_radii, degrees) # fit a polynomial for the current landmark\n",
    "\n",
    "        coefficients.append(coef) # append current coefficient array to list coefficients\n",
    "    \n",
    "    ##################################################################\n",
    "    #################### MODEL THEORETICAL LEAVES ####################\n",
    "    ##################################################################\n",
    "\n",
    "    plt.figure(figsize=(20,20)) # set figure size\n",
    "\n",
    "    ##############################################################\n",
    "    #################### CALCULATE NEW ANGLES ####################\n",
    "    ##############################################################\n",
    "\n",
    "    num_spaces = new_lobe_num + 1 # the number of divisions is 1 + the number of lobes\n",
    "    angle_val = 180/num_spaces # divide 180 by the number of spaces between lobes\n",
    "\n",
    "    angle_list = [] # a list to store the desired angles\n",
    "    ang = 90 # a counter, starting at 90\n",
    "\n",
    "    while ang < 270: # while the counter is < 270\n",
    "\n",
    "        ang += angle_val # add the angular value to counter\n",
    "        angle_list.append(ang) # append the angle\n",
    "\n",
    "    angle_list = angle_list[0:-1] # remove the last list element\n",
    "\n",
    "    ################################################################\n",
    "    #################### INTERPOLATE NEW ANGLES ####################\n",
    "    ################################################################\n",
    "\n",
    "    inter_angles = [] # a list to store the interpolated angles\n",
    "\n",
    "    for i in range(len(polar_lobe_angles)-1): # always starting on the angle up to the end of the interval\n",
    "\n",
    "        start_ind = i  # start index\n",
    "        end_ind = i + 1 # end index\n",
    "        interval_points = np.linspace(polar_lobe_angles[start_ind], polar_lobe_angles[end_ind], num_inter_points) # interpolate\n",
    "        inter_angles.append( interval_points[0:-1].tolist() ) # remove last element, because we START on the angle\n",
    "\n",
    "    inter_angles = [item for sublist in inter_angles for item in sublist] # flatten the list of lists\n",
    "\n",
    "    # find the indices closest to the angles\n",
    "\n",
    "    index_list = [] # create a list to store the indices closest to each desired angle\n",
    "\n",
    "    for i in range(len(angle_list)): # for each desired angle\n",
    "\n",
    "        curr_ang = angle_list[i] # select current angle\n",
    "\n",
    "        distances = [] # create list to store distances from each interpolated point\n",
    "\n",
    "        for j in range(len(inter_angles)): # for each interpolated point\n",
    "\n",
    "            distances.append(abs(curr_ang - inter_angles[j])) # append the distance of the interpolated point to the desired angle\n",
    "\n",
    "        index_list.append(distances.index(min(distances))) # append the index of the minimum value\n",
    "\n",
    "    # now that we have the indices to use for the deisred angles, we need to create\n",
    "    # lists of interpolated points for each pseudolandmark, just like we did for the tip points\n",
    "\n",
    "    interpolated_angles = [] # list of lists, for each pseudolandmark, the interpolated angles\n",
    "\n",
    "    for j in range(len(polar_angles[0])): # for the number of pseudolandmarks\n",
    "\n",
    "        pseudo_angle_list = [] # list to store the angles of the current pseudolandmark\n",
    "\n",
    "        for i in range(len(polar_angles)): # for each lobe\n",
    "\n",
    "            pseudo_angle_list.append(polar_angles[i][j]) # append the angle of the pseudolandmark for the current lobe\n",
    "\n",
    "        pseudo_angle_list.insert(0,0) # add 0 at beginning for intervals \n",
    "        pseudo_angle_list.append(360) # add 360 at end for intervals \n",
    "\n",
    "        inter_pseudo_angles = []\n",
    "\n",
    "        for k in range(len(pseudo_angle_list)-1): # always starting on the angle up to the end of the interval\n",
    "\n",
    "            start_ind = k  # start index\n",
    "            end_ind = k + 1 # end index\n",
    "            interval_points = np.linspace(pseudo_angle_list[start_ind], pseudo_angle_list[end_ind], num_inter_points) # interpolate\n",
    "            inter_pseudo_angles.append( interval_points[0:-1].tolist() ) # remove last element, because we START on the angle\n",
    "\n",
    "        inter_pseudo_angles = [item for sublist in inter_pseudo_angles for item in sublist] # flatten the list of lists\n",
    "\n",
    "        interpolated_angles.append(inter_pseudo_angles) # append the interpolated angles for the current pseudolandmark\n",
    "\n",
    "    #############################################################\n",
    "    #################### MODEL THE NEW LOBES ####################\n",
    "    #############################################################\n",
    "\n",
    "    # coefficients: a list of arrays of coefficients of model for each pseudolandmark\n",
    "    # interpolated_angles: a list of lists of interpolated angles for each pseudolandmark\n",
    "    # index_list: a list of indices for the desired angles\n",
    "\n",
    "    modeled_angles = [] # a list of lists, for each pseudolandmark, the modeled angle values\n",
    "    modeled_radii = [] # a list of lists, for each pseudolandmark, the modeled radius values\n",
    "\n",
    "    for i in range(len(coefficients)): # for each pseudolandmark\n",
    "\n",
    "        current_angles = [] # list of current angles to model for the current psuedolandmark\n",
    "\n",
    "        for j in range(len(index_list)): # for each index\n",
    "\n",
    "            current_angles.append(interpolated_angles[i][index_list[j]]) # append the associated pseudolandmark angle\n",
    "\n",
    "        coef = coefficients[i] # the coefficients of the model for the current pseudolandmark\n",
    "        polyfunc = np.poly1d(coef) # create polynomial functio n\n",
    "        current_radii = polyfunc(current_angles) # get radii of current angles\n",
    "\n",
    "        modeled_angles.append(current_angles) # append modeled angles for each pseudolandmark to list\n",
    "        modeled_radii.append(current_radii.tolist()) # append modeled radii for each pseudolandmark to list\n",
    "\n",
    "    #####################################################\n",
    "    #################### MAKE A PLOT ####################\n",
    "    #####################################################\n",
    "\n",
    "    curr_theor_xvals = [] # current theoretical x vals\n",
    "    curr_theor_yvals = [] # current theoretical y vals\n",
    "    \n",
    "    for i in range(len(modeled_angles)):\n",
    "\n",
    "        x = modeled_radii[i]*np.sin(np.radians(modeled_angles[i]))\n",
    "        y = modeled_radii[i]*np.cos(np.radians(modeled_angles[i]))\n",
    "        \n",
    "        curr_theor_xvals.append(x) # append current theoretical x val\n",
    "        curr_theor_yvals.append(y) # append current theoretical y val\n",
    "\n",
    "    plt.plot(curr_theor_xvals,curr_theor_yvals, lw=5)\n",
    "    plt.fill(curr_theor_xvals,curr_theor_yvals, alpha=0.03)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plot_name = population + \" \" + individual + \", node\" + node_id + \" of \" + str(total_num)\n",
    "    plt.suptitle(plot_name, fontsize=48)\n",
    "    plot_file = plot_path + file[:-4] + \".jpg\"\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    \n",
    "    theor_xvals.append(curr_theor_xvals) # append theoretical x vals for current leaf\n",
    "    theor_yvals.append(curr_theor_yvals) # append theoretical y vals for current leaf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revise the plots with modeled leaves and eliminate the leaves that are poorly modeled before continuing the analysis. Make sure to eliminate both landmark and ouline files in your folders, otherwide you will get an error. After eliminating the poorly modeled leaves, run the script again from \"GET FILE NAMES OF LEAVES\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZE MODELED LEAVES AND MORPHOSPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphospace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#################### REFORMAT DATA TO PLOT ####################\n",
    "###############################################################\n",
    "\n",
    "num_leaves = len(theor_xvals) # get number of leaves (list of lists)\n",
    "\n",
    "num_pts = len(theor_xvals[0]) # get number of points (list of arrays)\n",
    "\n",
    "new_lobe_num = len(theor_xvals[0][0]) # the new lobe number (array of new_lobe_num)\n",
    "\n",
    "theor_arr_x = np.zeros( (num_leaves, new_lobe_num*num_pts) )  # create empty arrays to fill\n",
    "theor_arr_y = np.zeros( (num_leaves, new_lobe_num*num_pts) ) \n",
    "\n",
    "for i in range(num_leaves):\n",
    "    \n",
    "    for j in range(num_pts):\n",
    "        \n",
    "        for k in range(new_lobe_num):\n",
    "            \n",
    "            theor_arr_x[i, (k*num_pts) + j] = theor_xvals[i][j][k] # place data in empty arrays\n",
    "            theor_arr_y[i, (k*num_pts) + j] = theor_yvals[i][j][k]\n",
    "\n",
    "theor_arr = np.column_stack((theor_arr_x, theor_arr_y)) # stack x and y values by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#################### FIND PERCENT VARIANCE FOR EACH PC ####################\n",
    "###########################################################################\n",
    "\n",
    "# set number of PCs to number of samples, so we can see the percent variance for all PCs\n",
    "test_pca = PCA(n_components=num_leaves)\n",
    "\n",
    "# fit a PCA\n",
    "test_PCs = test_pca.fit_transform(theor_arr) \n",
    "\n",
    "# print out explained variance for each PC (a proportion)\n",
    "print(test_pca.explained_variance_ratio_) \n",
    "\n",
    "# print out cumulative variance explained by PC (a proportion)\n",
    "print(test_pca.explained_variance_ratio_.cumsum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#################### CALCULATE INVERSE MORPHOSPACE ####################\n",
    "#######################################################################\n",
    "\n",
    "# define number of desired PCs\n",
    "# for reconstruction, it's easier to just use 2\n",
    "# but we can use more PC values if desired\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# apply the sklearn pca function with desired number of components\n",
    "\n",
    "PCs = pca.fit_transform(theor_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#################### DEFINE MORPHOSPACE FUNCTION ####################\n",
    "#####################################################################\n",
    "\n",
    "# define a function to plot background morphospace\n",
    "# inputs: scale, determined by checking, to prevent shape overlap in the plot. \n",
    "# PC1_pad and PC2_pad, padding on each side of PC to add extra space to see morphospace\n",
    "# num_intervals, number of intervals to divide each PC axis as a grid\n",
    "# output: a morphospace plot\n",
    "\n",
    "def Morphospace(scale, PC1_pad, PC2_pad, num_intervals, PCs):\n",
    "\n",
    "    # get PC1 interval values\n",
    "    PC1_intervals = np.linspace( np.min(PCs[:,0]), np.max(PCs[:,0]) ,num_intervals)\n",
    "\n",
    "    # get PC2 interval values\n",
    "    PC2_intervals = np.linspace( np.min(PCs[:,1]), np.max(PCs[:,1]), num_intervals)\n",
    "\n",
    "    for i in PC1_intervals:\n",
    "\n",
    "        for j in PC2_intervals:\n",
    "\n",
    "            # perform inverse PCA\n",
    "\n",
    "            inv_new = pca.inverse_transform(np.array([i,j]))\n",
    "            \n",
    "            # extract x and y vals, first half and second half\n",
    "\n",
    "            inv_PC1_vals = inv_new[0:1995]\n",
    "            inv_PC2_vals = inv_new[1995:]\n",
    "\n",
    "            # scale so the shapes don't overlap, determine the right scale by checking\n",
    "\n",
    "            scaled_PC1_vals = inv_PC1_vals*scale\n",
    "            scaled_PC2_vals = -inv_PC2_vals*scale # the - here inverts the axis, so the leaves are pointing up not down\n",
    "\n",
    "            # translate to the PCA point position\n",
    "\n",
    "            trans_PC1_vals = scaled_PC1_vals + i\n",
    "            trans_PC2_vals = scaled_PC2_vals + j\n",
    "\n",
    "            # plot out the results to make sure it is correct\n",
    "\n",
    "            plt.fill(trans_PC1_vals, trans_PC2_vals, c=\"gray\", alpha=0.3)\n",
    "            plt.xlim( (np.min(PCs[:,0])-PC1_pad, np.max(PCs[:,0])+PC1_pad) )\n",
    "            plt.ylim( (np.min(PCs[:,1])-PC2_pad, np.max(PCs[:,1])+PC2_pad) )\n",
    "            plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discretize relative node into a categorical variable\n",
    "\n",
    "relnod_list = []\n",
    "\n",
    "for i in range(len(rel_list)):\n",
    "    \n",
    "    curr_val = rel_list[i]\n",
    "    \n",
    "    if curr_val <= 0.1:\n",
    "        relnod_list.append(\"0.1\")\n",
    "    elif curr_val <= 0.2:\n",
    "        relnod_list.append(\"0.2\")\n",
    "    elif curr_val <= 0.3:\n",
    "        relnod_list.append(\"0.3\")\n",
    "    elif curr_val <= 0.4:\n",
    "        relnod_list.append(\"0.4\")\n",
    "    elif curr_val <= 0.5:\n",
    "        relnod_list.append(\"0.5\")\n",
    "    elif curr_val <= 0.6:\n",
    "        relnod_list.append(\"0.6\")\n",
    "    elif curr_val <= 0.7:\n",
    "        relnod_list.append(\"0.7\")\n",
    "    elif curr_val <= 0.8:\n",
    "        relnod_list.append(\"0.8\")\n",
    "    elif curr_val <= 0.9:\n",
    "        relnod_list.append(\"0.9\")\n",
    "    elif curr_val <= 1.0:\n",
    "        relnod_list.append(\"1.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#################### SET PARAMETERS AND PLOT ####################\n",
    "#################################################################\n",
    "\n",
    "# scale, determined by checking, to prevent shape overlap\n",
    "scale = 0.54\n",
    "\n",
    "# padding on each side of PC to add extra space to see morphospace\n",
    "PC1_pad = 1\n",
    "PC2_pad = 1\n",
    "\n",
    "# number of intervals to divide each PC axis\n",
    "num_intervals = 10\n",
    "\n",
    "# set the relative scale of text in the plot\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "# set the datapoint size\n",
    "pt_size = 500 \n",
    "\n",
    "# FACTORS TO COLOR SCATTERPLOT BY\n",
    "# pop_list = population\n",
    "# ind_list = individual\n",
    "# tot_list = total leaf number\n",
    "# nod_list = node number\n",
    "# rel_list = relative node value\n",
    "# relnod_list = relative node value as categorical variable\n",
    "# lob_list = lobe number\n",
    "\n",
    "# ACCESSION PLOT\n",
    "# set figure size\n",
    "plt.figure(figsize=(30,20))\n",
    "# apply morphospace function\n",
    "Morphospace(scale, PC1_pad, PC2_pad, num_intervals, PCs)\n",
    "#define color factor\n",
    "color_factor = pop_list\n",
    "#define palette\n",
    "palette_pop=(\"#481567\", \"#000000\", \"#453781\", \"#33638d\", \"#238a8d\", \"#29af7f\", \"#e2e068\", \"#73d055\", \"#ffe32e\")\n",
    "# plot the scatter plot\n",
    "sns.scatterplot(x=PCs[:,0], y=PCs[:,1], hue=color_factor, s=pt_size, palette=palette_pop)\n",
    "# move the legend outside the plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, frameon=False)\n",
    "# save the figure if you want\n",
    "#plt.savefig(\"morpho_acc.pdf\", format=\"pdf\")\n",
    "\n",
    "# LEAFLET PLOT\n",
    "# set figure size\n",
    "plt.figure(figsize=(30,20))\n",
    "# apply morphospace function\n",
    "Morphospace(scale, PC1_pad, PC2_pad, num_intervals, PCs)\n",
    "#define color factor\n",
    "color_factor = lob_list\n",
    "#define palette\n",
    "palette_lob=sns.color_palette(\"viridis_r\", 4)\n",
    "# plot the scatter plot\n",
    "sns.scatterplot(x=PCs[:,0], y=PCs[:,1], hue=color_factor, s=pt_size, palette=palette_lob)\n",
    "# move the legend outside the plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, frameon=False)\n",
    "# save the figure if you want\n",
    "#plt.savefig(\"morpho_leaflet.pdf\", format=\"pdf\")\n",
    "\n",
    "# RELATIVE NODE PLOT\n",
    "# set figure size\n",
    "plt.figure(figsize=(30,20))\n",
    "# apply morphospace function\n",
    "Morphospace(scale, PC1_pad, PC2_pad, num_intervals, PCs)\n",
    "#define color factor\n",
    "color_factor = relnod_list\n",
    "#define palette\n",
    "palette_relnod=(\"#3cbb75\", \"#20a387\", \"#2d708e\", \"#39568c\",\"#404788\", \"#481567\",\"#b8de29\", \"#73d055\", \"#238a8d\")\n",
    "# plot the scatter plot\n",
    "sns.scatterplot(x=PCs[:,0], y=PCs[:,1], hue=color_factor, s=pt_size, palette=palette_relnod)\n",
    "# move the legend outside the plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, frameon=False);\n",
    "# save the figure if you want\n",
    "#plt.savefig(\"morpho_relative_node.pdf\", format=\"pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#################### FIND OUTLIERS ####################\n",
    "#######################################################\n",
    "\n",
    "# first, convert the list of names to an array\n",
    "cannabis_arr = np.array(cannabis_leaves)\n",
    "\n",
    "# example: find all leaves with PC2 value greater than 2\n",
    "print(\"leaves greater than PC2 = 2\", cannabis_arr[PCs[:,1]>2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the PC1 and PC2\n",
    "PC1 = test_PCs[:, 0]\n",
    "PC2 = test_PCs[:, 1]\n",
    "acc=pop_list \n",
    "relnd=relnod_list \n",
    "lobe=lob_list\n",
    "\n",
    "# make a dataframe\n",
    "data = pd.DataFrame({'PC1': PC1, 'PC2': PC2, 'Acc': acc, 'Rel': relnd, 'Lobe': lobe })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask population levels\n",
    "AM15_PC1 = data[\"PC1\"][data[\"Acc\"]==\"AM15\"]\n",
    "BNG_PC1 = data[\"PC1\"][data[\"Acc\"]==\"BNG\"]\n",
    "FUT75_PC1 = data[\"PC1\"][data[\"Acc\"]==\"FUT75\"]\n",
    "HU1_PC1 = data[\"PC1\"][data[\"Acc\"]==\"HU1\"]\n",
    "IKL_PC1 = data[\"PC1\"][data[\"Acc\"]==\"IKL\"]\n",
    "IK_PC1 = data[\"PC1\"][data[\"Acc\"]==\"IK\"]\n",
    "MAR_PC1 = data[\"PC1\"][data[\"Acc\"]==\"MAR\"]\n",
    "MN9_PC1 = data[\"PC1\"][data[\"Acc\"]==\"MN9\"]\n",
    "RO1_PC1 = data[\"PC1\"][data[\"Acc\"]==\"RO1\"]\n",
    "\n",
    "AM15_PC2 = data[\"PC2\"][data[\"Acc\"]==\"AM15\"]\n",
    "BNG_PC2 = data[\"PC2\"][data[\"Acc\"]==\"BNG\"]\n",
    "FUT75_PC2 = data[\"PC2\"][data[\"Acc\"]==\"FUT75\"]\n",
    "HU1_PC2 = data[\"PC2\"][data[\"Acc\"]==\"HU1\"]\n",
    "IKL_PC2 = data[\"PC2\"][data[\"Acc\"]==\"IKL\"]\n",
    "IK_PC2 = data[\"PC2\"][data[\"Acc\"]==\"IK\"]\n",
    "MAR_PC2 = data[\"PC2\"][data[\"Acc\"]==\"MAR\"]\n",
    "MN9_PC2 = data[\"PC2\"][data[\"Acc\"]==\"MN9\"]\n",
    "RO1_PC2 = data[\"PC2\"][data[\"Acc\"]==\"RO1\"]\n",
    "\n",
    "# Kruskal-Wallis test\n",
    "print(\"Kruskal-Wallis test for accession~PC1:\", stats.kruskal(AM15_PC1, BNG_PC1, FUT75_PC1, HU1_PC1, IKL_PC1, IK_PC1, MAR_PC1, MN9_PC1, RO1_PC1))\n",
    "print(\"Kruskal-Wallis test for accession~PC2:\",stats.kruskal(AM15_PC2, BNG_PC2, FUT75_PC2, HU1_PC2, IKL_PC2, IK_PC2, MAR_PC2, MN9_PC2, RO1_PC2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask lobe levels\n",
    "L3_PC1 = data[\"PC1\"][data[\"Lobe\"]==3]\n",
    "L5_PC1 = data[\"PC1\"][data[\"Lobe\"]==5]\n",
    "L7_PC1 = data[\"PC1\"][data[\"Lobe\"]==7]\n",
    "L9_PC1 = data[\"PC1\"][data[\"Lobe\"]==9]\n",
    "\n",
    "L3_PC2 = data[\"PC2\"][data[\"Lobe\"]==3]\n",
    "L5_PC2 = data[\"PC2\"][data[\"Lobe\"]==5]\n",
    "L7_PC2 = data[\"PC2\"][data[\"Lobe\"]==7]\n",
    "L9_PC2 = data[\"PC2\"][data[\"Lobe\"]==9]\n",
    "\n",
    "# Kruskal-Wallis test\n",
    "print(\"Kruskal-Wallis test for lobe~PC1:\", stats.kruskal(L3_PC1, L5_PC1, L7_PC1, L9_PC1))\n",
    "print(\"Kruskal-Wallis test for lobe~PC2:\",stats.kruskal(L3_PC2, L5_PC2, L7_PC2, L9_PC2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mask relative node levels\n",
    "R2_PC1 = data[\"PC1\"][data[\"Rel\"]==\"0.2\"]\n",
    "R3_PC1 = data[\"PC1\"][data[\"Rel\"]==\"0.3\"]\n",
    "R4_PC1 = data[\"PC1\"][data[\"Rel\"]==\"0.4\"]\n",
    "R5_PC1 = data[\"PC1\"][data[\"Rel\"]==\"0.5\"]\n",
    "R6_PC1 = data[\"PC1\"][data[\"Rel\"]==\"0.6\"]\n",
    "R7_PC1 = data[\"PC1\"][data[\"Rel\"]==\"0.7\"]\n",
    "R8_PC1 = data[\"PC1\"][data[\"Rel\"]==\"0.9\"]\n",
    "R9_PC1 = data[\"PC1\"][data[\"Rel\"]==\"0.9\"]\n",
    "R10_PC1 = data[\"PC1\"][data[\"Rel\"]==\"1.0\"]\n",
    "\n",
    "R2_PC2 = data[\"PC2\"][data[\"Rel\"]==\"0.2\"]\n",
    "R3_PC2 = data[\"PC2\"][data[\"Rel\"]==\"0.3\"]\n",
    "R4_PC2 = data[\"PC2\"][data[\"Rel\"]==\"0.4\"]\n",
    "R5_PC2 = data[\"PC2\"][data[\"Rel\"]==\"0.5\"]\n",
    "R6_PC2 = data[\"PC2\"][data[\"Rel\"]==\"0.6\"]\n",
    "R7_PC2 = data[\"PC2\"][data[\"Rel\"]==\"0.7\"]\n",
    "R8_PC2 = data[\"PC2\"][data[\"Rel\"]==\"0.9\"]\n",
    "R9_PC2 = data[\"PC2\"][data[\"Rel\"]==\"0.9\"]\n",
    "R10_PC2 = data[\"PC2\"][data[\"Rel\"]==\"1.0\"]\n",
    "\n",
    "# Kruskal-Wallis test\n",
    "print(\"Kruskal-Wallis test for relative node~PC1:\", stats.kruskal(R2_PC1, R3_PC1, R4_PC1, R5_PC1, R6_PC1, R7_PC1, R8_PC1, R9_PC1, R10_PC1))\n",
    "print(\"Kruskal-Wallis test for relative node~PC2:\", stats.kruskal(R2_PC2, R3_PC2, R4_PC2, R5_PC2, R6_PC2, R7_PC2, R8_PC2, R9_PC2, R10_PC2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize leaves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###### BY ACCESSION ######\n",
    "##########################\n",
    "\n",
    "# convert list to arr\n",
    "pop_arr = np.array(pop_list)\n",
    "\n",
    "# get unique factor levels to plot\n",
    "pop_names = np.unique(pop_arr)\n",
    "\n",
    "# set figure size\n",
    "plt.figure(figsize=(10,20))\n",
    "\n",
    "# parameters\n",
    "fs = 20\n",
    "plot_colors = [\"#481567\", \"#000000\", \"#453781\", \"#33638d\", \"#238a8d\", \"#29af7f\", \"#e2e068\", \"#73d055\", \"#ffe32e\"]\n",
    "outline_col = \"k\"\n",
    "\n",
    "for i in range(len(pop_names)):\n",
    "\n",
    "    # get current pop to plot\n",
    "    curr_pop = pop_names[i]\n",
    "\n",
    "    # mask pop to plot\n",
    "    pop_vals = theor_arr[pop_arr==curr_pop]\n",
    "\n",
    "    # get mean vals\n",
    "    pop_mean = np.mean(pop_vals, axis=0)\n",
    "\n",
    "    # get x and y vals\n",
    "    popx = pop_mean[0:int(len(pop_mean)/2)]\n",
    "    popy = pop_mean[int(len(pop_mean)/2):]\n",
    "\n",
    "    # plot\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.plot(-popx, -popy, c=outline_col)\n",
    "    plt.fill(-popx, -popy, c=plot_colors[i])\n",
    "    plt.title(curr_pop, fontsize=fs)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.suptitle(\"ACCESSION\", fontsize=fs);\n",
    "\n",
    "#plt.savefig(\"plot_ACCESSION.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################\n",
    "###### BY LEAFLET NUMBER ######\n",
    "############################\n",
    "\n",
    "## convert lobe number to string\n",
    "lob_str = [str(x) for x in lob_list]\n",
    "\n",
    "# convert list to arr\n",
    "lob_arr = np.array(lob_str)\n",
    "\n",
    "# get unique factor levels to plot\n",
    "lob_names = np.unique(lob_arr)\n",
    "\n",
    "# set figure size\n",
    "plt.figure(figsize=(10,20))\n",
    "\n",
    "# parameters\n",
    "fs = 20\n",
    "plot_colors = [\"#73d055\", \"#20a387\", \"#287d8e\", \"#404788\"]\n",
    "outline_col = \"k\"\n",
    "\n",
    "for i in range(len(lob_names)):\n",
    "\n",
    "    # get current lob to plot\n",
    "    curr_lob = lob_names[i]\n",
    "\n",
    "    # mask lobe number to plot\n",
    "    lob_vals = theor_arr[lob_arr==curr_lob]\n",
    "\n",
    "    # get mean vals\n",
    "    lob_mean = np.mean(lob_vals, axis=0)\n",
    "\n",
    "    # get x and y vals\n",
    "    lobx = lob_mean[0:int(len(lob_mean)/2)]\n",
    "    loby = lob_mean[int(len(lob_mean)/2):]\n",
    "\n",
    "    # plot\n",
    "    plt.subplot(4,1,i+1)\n",
    "    plt.plot(-lobx, -loby, c=outline_col)\n",
    "    plt.fill(-lobx, -loby, c=plot_colors[i])\n",
    "    plt.title(curr_lob, fontsize=fs)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.suptitle(\"LEAFLET NUMBER\", fontsize=fs);\n",
    "\n",
    "#plt.savefig(\"plot_LEAFLETNUMBER.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "###### BY RELATIVE NODE NUMBER ######\n",
    "#####################################\n",
    "\n",
    "# convert list to arr\n",
    "rel_arr = np.array(relnod_list)\n",
    "\n",
    "# get unique factor levels to plot\n",
    "rel_names = np.unique(rel_arr)\n",
    "\n",
    "# set figure size\n",
    "plt.figure(figsize=(10,20))\n",
    "\n",
    "# parameters\n",
    "fs = 20\n",
    "plot_color = [\"#b8de29\", \"#73d055\", \"#3cbb75\",\"#20a387\", \"#238a8d\", \"#2d708e\", \"#39568c\",\"#404788\", \"#481567\"]\n",
    "outline_col = \"k\"\n",
    "\n",
    "for i in range(len(rel_names)):\n",
    "\n",
    "    # get current relative node to plot\n",
    "    curr_rel = rel_names[i]\n",
    "\n",
    "    # mask relative node to plot\n",
    "    rel_vals = theor_arr[rel_arr==curr_rel]\n",
    "\n",
    "    # get mean vals\n",
    "    rel_mean = np.mean(rel_vals, axis=0)\n",
    "\n",
    "    # get x and y vals\n",
    "    relx = rel_mean[0:int(len(rel_mean)/2)]\n",
    "    rely = rel_mean[int(len(rel_mean)/2):]\n",
    "\n",
    "    # plot\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.plot(-relx, -rely, c=outline_col)\n",
    "    plt.fill(-relx, -rely, c=plot_color[i])\n",
    "    plt.title(curr_rel, fontsize=fs)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.suptitle(\"RELATIVE NODE NUMBER\", fontsize=fs);\n",
    "\n",
    "#plt.savefig(\"plot_RELATIVENODE.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR DISCRIMINANT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By accession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x is the features we use to predict factor\n",
    "## they are the modeled x and y coordinate values\n",
    "x_pop = theor_arr\n",
    "\n",
    "## y is the factor we are predicting\n",
    "## in this case it is population\n",
    "y_pop = pop_list\n",
    "\n",
    "## we create an LDA model for population from coordiate values\n",
    "## and then we fit the model\n",
    "pop_model = LinearDiscriminantAnalysis()\n",
    "pop_model.fit(x_pop, y_pop)\n",
    "\n",
    "## calculate the LDs from the model\n",
    "pop_LDs = pop_model.fit(x_pop, y_pop).transform(x_pop)\n",
    "\n",
    "## create a df to plot the data\n",
    "popLD_df = pd.DataFrame({'LD1':pop_LDs[:,0], \n",
    "                      'LD2':pop_LDs[:,1], \n",
    "                      'pop':pop_list})\n",
    "\n",
    "## define the palette\n",
    "palette_pop=(\"#481567\", \"#000000\", \"#453781\", \"#33638d\", \"#238a8d\", \"#29af7f\", \"#e2e068\", \"#73d055\", \"#ffe32e\")\n",
    "\n",
    "## define figure size\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "## plot\n",
    "ax=sns.scatterplot(data=popLD_df, x=\"LD1\", y=\"LD2\", hue=\"pop\", s=300, palette=palette_pop, legend=True)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.set_xlabel(\"LD1\", fontsize=30)\n",
    "ax.set_ylabel(\"LD2\", fontsize=30)\n",
    "sns.move_legend(ax, \"lower center\", bbox_to_anchor=(0.5, 1.0), fontsize= 30, ncol=5, \n",
    "                markerscale=3, title=\"Accession\", frameon=False)\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='30') \n",
    "#plt.savefig(\"LDA_acc.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By leaflet number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x is the features we use to predict factor\n",
    "## they are the modeled x and y coordinate values\n",
    "x_lob = theor_arr\n",
    "\n",
    "## y is the factor we are predicting\n",
    "## in this case it is lobe number\n",
    "## convert lobe number to string\n",
    "y_lob = [str(x) for x in lob_list]\n",
    "\n",
    "## we create an LDA model for lobe number from coordiate values\n",
    "## and then we fit the model\n",
    "lob_model = LinearDiscriminantAnalysis()\n",
    "lob_model.fit(x_lob, y_lob)\n",
    "\n",
    "## calculate the LDs from the model\n",
    "lob_LDs = lob_model.fit(x_lob, y_lob).transform(x_lob)\n",
    "\n",
    "## create a df to plot the data\n",
    "lobLD_df = pd.DataFrame({'LD1':lob_LDs[:,0], \n",
    "                      'LD2':lob_LDs[:,1], \n",
    "                      'lob':y_lob})\n",
    "\n",
    "## Define the palette\n",
    "palette_lob=(\"#20a387\", \"#287d8e\", \"#73d055\", \"#404788\")\n",
    "\n",
    "## Define the figure size\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "## Plot\n",
    "ax=sns.scatterplot(data=lobLD_df, x=\"LD1\", y=\"LD2\", hue=\"lob\", s=300, palette=palette_lob, legend=True)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.set_xlabel(\"LD1\", fontsize=30)\n",
    "ax.set_ylabel(\"LD2\", fontsize=30)\n",
    "sns.move_legend(ax, \"lower center\", bbox_to_anchor=(0.5, 1), fontsize= 30, ncol=4, \n",
    "                markerscale=3, title=\"Leaflet number\", frameon=False)\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='30') \n",
    "#plt.savefig(\"LDA_leaflet.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By relative node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## x is the features we use to predict factor\n",
    "## they are the modeled x and y coordinate values\n",
    "x_rel = theor_arr\n",
    "\n",
    "## y is the factor we are predicting\n",
    "## in this case it is relative node number\n",
    "y_rel = relnod_list\n",
    "\n",
    "## we create an LDA model for relative node number from coordiate values\n",
    "## and then we fit the model\n",
    "rel_model = LinearDiscriminantAnalysis()\n",
    "rel_model.fit(x_rel, y_rel)\n",
    "\n",
    "## calculate the LDs from the model\n",
    "rel_LDs = rel_model.fit(x_rel, y_rel).transform(x_rel)\n",
    "\n",
    "## create a df to plot the data\n",
    "relLD_df = pd.DataFrame({'LD1':rel_LDs[:,0], \n",
    "                      'LD2':rel_LDs[:,1], \n",
    "                      'rel':y_rel})\n",
    "\n",
    "## Define the palette\n",
    "palette_rel_node=(\"#3cbb75\", \"#20a387\", \"#2d708e\", \"#39568c\",\"#404788\", \"#481567\",\"#b8de29\", \"#73d055\", \"#238a8d\")\n",
    "\n",
    "## Define the figure size\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "## Plot\n",
    "ax=sns.scatterplot(data=relLD_df, x=\"LD1\", y=\"LD2\", hue=\"rel\", s=300, palette=palette_rel_node, legend=True)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.set_xlabel(\"LD1\", fontsize=30)\n",
    "ax.set_ylabel(\"LD2\", fontsize=30)\n",
    "sns.move_legend(ax, \"lower center\", bbox_to_anchor=(0.5, 1), fontsize= 30, ncol=5, \n",
    "                markerscale=2.5, title=\"Relative node number\", frameon=False)\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='30') \n",
    "#plt.savefig(\"LDA_rel.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION ON TRAIN (a) AND TEST (b) DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the indexes of where leaves with a and c and leaves with b and d are found and save them in a new list of \n",
    "# indexes for leaves ind_a and ind_b and check the length\n",
    "\n",
    "def find_ac(s, ch):\n",
    "    return [i for i, ltr in enumerate(s) if ltr == ch or ltr == 'c']\n",
    "\n",
    "def find_bd(s, ch):\n",
    "    return [i for i, ltr in enumerate(s) if ltr == ch or ltr == 'd']\n",
    "\n",
    "    \n",
    "leaf_a = find_ac(nodab_list, \"a\")\n",
    "leaf_b = find_bd(nodab_list, \"b\")\n",
    "#print(len(nodab_list))\n",
    "#print(len(leaf_a))\n",
    "#print(len(leaf_b))\n",
    "#print(len(leaf_a) + len(leaf_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the rows with the x and y coordinate values for with leaves a and b into a separate array.\n",
    "new_arr_a = theor_arr[leaf_a]\n",
    "new_arr_b = theor_arr[leaf_b]\n",
    "\n",
    "## Check the new structures of lists and arrays to make sure :)\n",
    "#print(len(leaf_a), len(leaf_b))\n",
    "#print(len(new_arr_a), len(new_arr_b))    \n",
    "#print(type(new_arr_a), type(new_arr_b) \n",
    "#print(new_arr_a.shape, new_arr_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Then we need to extract the data from the lists we will use as y test dataset\n",
    "\n",
    "pop_list_a = [pop_list[i] for i in leaf_a]\n",
    "pop_list_b = [pop_list[i] for i in leaf_b]\n",
    "#print(len(pop_list_a), len(pop_list_b)) \n",
    "\n",
    "relnod_list_a = [relnod_list[i] for i in leaf_a]\n",
    "relnod_list_b = [relnod_list[i] for i in leaf_b]\n",
    "#print(len(relnod_list_a), len(relnod_list_b)) \n",
    "\n",
    "lob_list_a = [lob_list[i] for i in leaf_a]\n",
    "lob_list_b = [lob_list[i] for i in leaf_b]\n",
    "#print(len(lob_list_a), len(lob_list_b)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on a and predict the b leaves and plot confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By accession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## x is the features we use to predict factor\n",
    "## they are the modeled x and y coordinate values\n",
    "x_a = new_arr_a\n",
    "x_b = new_arr_b\n",
    "\n",
    "## y is the factor we are predicting\n",
    "## in this case it is population\n",
    "y_a = pop_list_a\n",
    "y_b = pop_list_b\n",
    "\n",
    "## we create an LDA model for population from coordiate values\n",
    "## and then we fit the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(x_a, y_a)\n",
    "prediction_pop= model.predict(x_b)\n",
    "\n",
    "comparison_result = [x == y for x, y in zip(y_b, prediction_pop)]\n",
    "\n",
    "count_true = comparison_result.count(True)\n",
    "count_false = comparison_result.count(False)\n",
    "print(\"Count of True values for accession:\", count_true)\n",
    "print(\"Count of False values for accession:\", count_false)\n",
    "\n",
    "## Plot a confusion matrix\n",
    "\n",
    "# True values\n",
    "true_values = pop_list_b\n",
    "\n",
    "# Predicted values\n",
    "predicted_values = prediction_pop\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_values, predicted_values)\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"viridis\", square=True, annot_kws={\"fontsize\": 20}, cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12);\n",
    "#plt.savefig(\"confusion_accession.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By leaflet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x is the features we use to predict factor\n",
    "## they are the modeled x and y coordinate values\n",
    "x_a = new_arr_a\n",
    "x_b = new_arr_b\n",
    "\n",
    "## y is the factor we are predicting\n",
    "## in this case it is population\n",
    "y_a = lob_list_a\n",
    "y_b = lob_list_b\n",
    "\n",
    "## we create an LDA model for population from coordiate values\n",
    "## and then we fit the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(x_a, y_a)\n",
    "prediction_lob= model.predict(x_b)\n",
    "\n",
    "comparison_result = [x == y for x, y in zip(y_b, prediction_lob)]\n",
    "\n",
    "count_true = comparison_result.count(True)\n",
    "count_false = comparison_result.count(False)\n",
    "print(\"Count of True values for leaflet number:\", count_true)\n",
    "print(\"Count of False values for leaflet number:\", count_false)\n",
    "\n",
    "\n",
    "## Plot a confusion matrix\n",
    "\n",
    "# True values\n",
    "true_values = lob_list_b\n",
    "\n",
    "# Predicted values\n",
    "predicted_values = prediction_lob\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_values, predicted_values)\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"viridis\", square=True, annot_kws={\"fontsize\": 20}, cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12);\n",
    "#plt.savefig(\"confusion_leaflet.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By relative node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## x is the features we use to predict factor\n",
    "## they are the modeled x and y coordinate values\n",
    "x_a = new_arr_a\n",
    "x_b = new_arr_b\n",
    "\n",
    "## y is the factor we are predicting\n",
    "## in this case it is population\n",
    "y_a = relnod_list_a\n",
    "y_b = relnod_list_b\n",
    "\n",
    "## we create an LDA model for population from coordiate values\n",
    "## and then we fit the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(x_a, y_a)\n",
    "prediction_relnod= model.predict(x_b)\n",
    "\n",
    "comparison_result = [x == y for x, y in zip(y_b, prediction_relnod)]\n",
    "\n",
    "count_true = comparison_result.count(True)\n",
    "count_false = comparison_result.count(False)\n",
    "print(\"Count of True values for relative node:\", count_true)\n",
    "print(\"Count of False values for relative node:\", count_false)\n",
    "\n",
    "\n",
    "## Plot a confusion matrix\n",
    "\n",
    "# True values\n",
    "true_values = relnod_list_b\n",
    "\n",
    "# Predicted values\n",
    "predicted_values = prediction_relnod\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_values, predicted_values)\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"viridis\", square=True, annot_kws={\"fontsize\": 20}, cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12);\n",
    "#plt.savefig(\"confusion_rel_node.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate Spearman Rank correlation and corresponding p-value\n",
    "rho, p = spearmanr(relnod_list_b, prediction_relnod)\n",
    "\n",
    "#print Spearman rank correlation and p-value\n",
    "print(rho)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
